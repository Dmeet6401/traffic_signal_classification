{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "import shutil\n",
        "import tensorflow \n",
        "from tensorflow.keras import Sequential,models\n",
        "from tensorflow.keras.layers import Dense,Flatten,BatchNormalization,Conv2D,MaxPooling2D, Dropout, GlobalAveragePooling2D,Activation\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "kvemnj07rtWL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-6Pj7e1FOQm3"
      },
      "outputs": [],
      "source": [
        "# from keras.optimizers import Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D,Dropout,GlobalAveragePooling2D\n",
        "from keras.metrics import categorical_crossentropy\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "# import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.image import imread\n",
        "import pathlib\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# %matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip training data\n",
        "from zipfile import ZipFile\n",
        "file_name = \"/content/drive/MyDrive/Colab Notebooks/Computer_Vision/classification/traffic_signal/train_test_data.zip\"\n",
        "with ZipFile(file_name, 'r') as zip:\n",
        "  zip.extractall('/content/drive/MyDrive/Colab Notebooks/Computer_Vision/classification/traffic_signal/traffic_data')\n",
        "  print('done')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCwj5i9eO0A9",
        "outputId": "36c906fd-e843-41a7-dbb2-654bd4ec6d2a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Unzip training data\n",
        "# from zipfile import ZipFile\n",
        "# file_name = \"/content/drive/MyDrive/Colab Notebooks/Computer_Vision/classification/traffic_signal/archive/traffic_Data/TEST.zip\"\n",
        "# with ZipFile(file_name, 'r') as zip:\n",
        "#   zip.extractall('/content/drive/MyDrive/Colab Notebooks/Computer_Vision/classification/traffic_signal/TEST')\n",
        "#   print('done')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cuP-Fpwjm-w",
        "outputId": "a4c8274a-6501-4ceb-d480-ceabdd5db04a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install split-folders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70y3eBvPO91u",
        "outputId": "8bff906a-d065-4efe-e8b9-9bed02685e53"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting split-folders\n",
            "  Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: split-folders\n",
            "Successfully installed split-folders-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import splitfolders\n",
        "# Split with a ratio.\n",
        "# To only split into training and validation set, set a tuple to `ratio`, i.e, `(.8, .2)`.\n",
        "splitfolders.ratio(\"/content/drive/MyDrive/Colab Notebooks/Computer_Vision/classification/traffic_signal/traffic_data/train_test_data/train\",\n",
        " output=\"/content/drive/MyDrive/Colab Notebooks/Computer_Vision/classification/traffic_signal/traffic_data/\",\n",
        "    # seed=1337, \n",
        "    ratio=(.8, .2), group_prefix=None, move=False) # default values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeBtl_5hPef-",
        "outputId": "bf7dc936-2837-40e1-b11a-2e6ee2cdd01e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying files: 3313 files [00:32, 100.47 files/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import splitfolders\n",
        "# # Split with a ratio.\n",
        "# # To only split into training and validation set, set a tuple to `ratio`, i.e, `(.8, .2)`.\n",
        "# splitfolders.ratio(\"/content/drive/MyDrive/Colab Notebooks/Computer_Vision/classification/traffic_signal/archive/traffic_Data/TEST/TEST\",\n",
        "#  output=\"/content/drive/MyDrive/Colab Notebooks/Computer_Vision/classification/traffic_signal\",\n",
        "#     # seed=1337, \n",
        "#     ratio=(.7, .3), group_prefix=None, move=False) # default values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKN5K715ibET",
        "outputId": "ee2b7ac4-1072-48ad-a055-c3c40d2c9704"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying files: 1994 files [18:23,  1.81 files/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOMoccNhlxY8",
        "outputId": "684f1c0c-a478-4e3c-f5d7-7f380b771fa2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir ='/content/drive/MyDrive/Colab Notebooks/Computer_Vision/classification/traffic_signal/traffic_data/train'\n",
        "val_dir = '/content/drive/MyDrive/Colab Notebooks/Computer_Vision/classification/traffic_signal/traffic_data/val'\n",
        "test_dir ='/content/drive/MyDrive/Colab Notebooks/Computer_Vision/classification/traffic_signal/traffic_data/train_test_data/validation'\n",
        "inf_dir='/content/drive/MyDrive/Colab Notebooks/Computer_Vision/classification/traffic_signal/traffic_data/train_test_data/test'"
      ],
      "metadata": {
        "id": "h8_T5wLVPyGt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_width=150\n",
        "img_height=150\n",
        "batch_size=128"
      ],
      "metadata": {
        "id": "ja1tRiX-RRgb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   rotation_range=30,\n",
        "                                   zoom_range=0.4,\n",
        "                                   horizontal_flip=True)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(train_dir,\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    # subset=\"training\",\n",
        "                                                    class_mode='sparse',\n",
        "                                                    target_size=(img_height, img_width))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmSRTdATRzC0",
        "outputId": "2a93e63b-aca2-48d9-9277-4423a0d5ad39"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2628 images belonging to 58 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   rotation_range=30,\n",
        "                                   zoom_range=0.4,\n",
        "                                   horizontal_flip=True)\n",
        "\n",
        "test_generator = train_datagen.flow_from_directory(test_dir,\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    # subset=\"training\",\n",
        "                                                    class_mode='sparse',\n",
        "                                                    target_size=(img_height, img_width))"
      ],
      "metadata": {
        "id": "uEj5oxJXR155",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "292dcf53-233f-49c7-fd58-9453fbd07b86"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 857 images belonging to 58 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   rotation_range=30,\n",
        "                                   zoom_range=0.4,\n",
        "                                   horizontal_flip=True)\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(val_dir,\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    # subset=\"training\",\n",
        "                                                    class_mode='sparse',\n",
        "                                                    target_size=(img_height, img_width))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pp7qJVhNUsdx",
        "outputId": "dbfbd867-69a4-4118-d495-e5e0a2171975"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 685 images belonging to 58 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ceating VGG16"
      ],
      "metadata": {
        "id": "7VuSVR4yrlP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_shape = 150\n",
        "conv_base = VGG16(\n",
        "    weights='imagenet',\n",
        "    include_top = False,\n",
        "    input_shape=(img_shape,img_shape,3)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1AnAjkQri2J",
        "outputId": "f68fb8ad-7573-4d0c-d599-9256c4eba0fa"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 4s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import BatchNormalization"
      ],
      "metadata": {
        "id": "2dP7zMl1VGiu"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(conv_base)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256,activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(128,activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(58,activation='softmax'))"
      ],
      "metadata": {
        "id": "dOO9wfVzsOon"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',loss = \"sparse_categorical_crossentropy\",metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "NHvFaVqnsQgB"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "filepath=\"/kaggle/working/VGG16_150-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]"
      ],
      "metadata": {
        "id": "IEFCs7aksSY3"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = models.load_model('../input/trafficsignmodified/VGG16_150-40-0.96_sparse.hdf5')"
      ],
      "metadata": {
        "id": "5GULA3Bl0f-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_generator,epochs=20,validation_data=val_generator,callbacks=callbacks_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiaDAOGDsU7J",
        "outputId": "1e6cf64f-84f8-41c7-b3c9-35ef560278f6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.2226 - accuracy: 0.9273\n",
            "Epoch 1: val_accuracy improved from 0.93869 to 0.94015, saving model to /kaggle/working/VGG16_150-01-0.94.hdf5\n",
            "21/21 [==============================] - 32s 2s/step - loss: 0.2226 - accuracy: 0.9273 - val_loss: 0.2131 - val_accuracy: 0.9401\n",
            "Epoch 2/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.1950 - accuracy: 0.9353\n",
            "Epoch 2: val_accuracy did not improve from 0.94015\n",
            "21/21 [==============================] - 28s 1s/step - loss: 0.1950 - accuracy: 0.9353 - val_loss: 0.3365 - val_accuracy: 0.9343\n",
            "Epoch 3/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.2131 - accuracy: 0.9387\n",
            "Epoch 3: val_accuracy did not improve from 0.94015\n",
            "21/21 [==============================] - 29s 1s/step - loss: 0.2131 - accuracy: 0.9387 - val_loss: 0.2290 - val_accuracy: 0.9343\n",
            "Epoch 4/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.2384 - accuracy: 0.9231\n",
            "Epoch 4: val_accuracy did not improve from 0.94015\n",
            "21/21 [==============================] - 29s 1s/step - loss: 0.2384 - accuracy: 0.9231 - val_loss: 0.2374 - val_accuracy: 0.9372\n",
            "Epoch 5/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.1986 - accuracy: 0.9395\n",
            "Epoch 5: val_accuracy did not improve from 0.94015\n",
            "21/21 [==============================] - 28s 1s/step - loss: 0.1986 - accuracy: 0.9395 - val_loss: 0.9661 - val_accuracy: 0.8234\n",
            "Epoch 6/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.1994 - accuracy: 0.9349\n",
            "Epoch 6: val_accuracy did not improve from 0.94015\n",
            "21/21 [==============================] - 28s 1s/step - loss: 0.1994 - accuracy: 0.9349 - val_loss: 0.2850 - val_accuracy: 0.9285\n",
            "Epoch 7/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.2065 - accuracy: 0.9338\n",
            "Epoch 7: val_accuracy did not improve from 0.94015\n",
            "21/21 [==============================] - 28s 1s/step - loss: 0.2065 - accuracy: 0.9338 - val_loss: 0.2583 - val_accuracy: 0.9343\n",
            "Epoch 8/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.1894 - accuracy: 0.9372\n",
            "Epoch 8: val_accuracy improved from 0.94015 to 0.95182, saving model to /kaggle/working/VGG16_150-08-0.95.hdf5\n",
            "21/21 [==============================] - 29s 1s/step - loss: 0.1894 - accuracy: 0.9372 - val_loss: 0.1842 - val_accuracy: 0.9518\n",
            "Epoch 9/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.1694 - accuracy: 0.9502\n",
            "Epoch 9: val_accuracy did not improve from 0.95182\n",
            "21/21 [==============================] - 28s 1s/step - loss: 0.1694 - accuracy: 0.9502 - val_loss: 3.6590 - val_accuracy: 0.8044\n",
            "Epoch 10/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.2143 - accuracy: 0.9319\n",
            "Epoch 10: val_accuracy did not improve from 0.95182\n",
            "21/21 [==============================] - 28s 1s/step - loss: 0.2143 - accuracy: 0.9319 - val_loss: 0.2366 - val_accuracy: 0.9460\n",
            "Epoch 11/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.2169 - accuracy: 0.9334\n",
            "Epoch 11: val_accuracy did not improve from 0.95182\n",
            "21/21 [==============================] - 28s 1s/step - loss: 0.2169 - accuracy: 0.9334 - val_loss: 0.2195 - val_accuracy: 0.9387\n",
            "Epoch 12/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.2074 - accuracy: 0.9346\n",
            "Epoch 12: val_accuracy did not improve from 0.95182\n",
            "21/21 [==============================] - 28s 1s/step - loss: 0.2074 - accuracy: 0.9346 - val_loss: 0.2818 - val_accuracy: 0.9285\n",
            "Epoch 13/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.2128 - accuracy: 0.9334\n",
            "Epoch 13: val_accuracy did not improve from 0.95182\n",
            "21/21 [==============================] - 28s 1s/step - loss: 0.2128 - accuracy: 0.9334 - val_loss: 0.2173 - val_accuracy: 0.9416\n",
            "Epoch 14/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.1951 - accuracy: 0.9460\n",
            "Epoch 14: val_accuracy did not improve from 0.95182\n",
            "21/21 [==============================] - 28s 1s/step - loss: 0.1951 - accuracy: 0.9460 - val_loss: 0.8563 - val_accuracy: 0.9299\n",
            "Epoch 15/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.2044 - accuracy: 0.9384\n",
            "Epoch 15: val_accuracy did not improve from 0.95182\n",
            "21/21 [==============================] - 28s 1s/step - loss: 0.2044 - accuracy: 0.9384 - val_loss: 0.9378 - val_accuracy: 0.9474\n",
            "Epoch 16/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.2015 - accuracy: 0.9365\n",
            "Epoch 16: val_accuracy did not improve from 0.95182\n",
            "21/21 [==============================] - 28s 1s/step - loss: 0.2015 - accuracy: 0.9365 - val_loss: 737.5610 - val_accuracy: 0.9270\n",
            "Epoch 17/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.1952 - accuracy: 0.9376\n",
            "Epoch 17: val_accuracy did not improve from 0.95182\n",
            "21/21 [==============================] - 28s 1s/step - loss: 0.1952 - accuracy: 0.9376 - val_loss: 0.2697 - val_accuracy: 0.9358\n",
            "Epoch 18/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.1875 - accuracy: 0.9403\n",
            "Epoch 18: val_accuracy did not improve from 0.95182\n",
            "21/21 [==============================] - 28s 1s/step - loss: 0.1875 - accuracy: 0.9403 - val_loss: 0.2849 - val_accuracy: 0.9212\n",
            "Epoch 19/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.1829 - accuracy: 0.9384\n",
            "Epoch 19: val_accuracy did not improve from 0.95182\n",
            "21/21 [==============================] - 28s 1s/step - loss: 0.1829 - accuracy: 0.9384 - val_loss: 0.3593 - val_accuracy: 0.9489\n",
            "Epoch 20/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.1844 - accuracy: 0.9448\n",
            "Epoch 20: val_accuracy did not improve from 0.95182\n",
            "21/21 [==============================] - 28s 1s/step - loss: 0.1844 - accuracy: 0.9448 - val_loss: 0.2571 - val_accuracy: 0.9285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/MyDrive/Colab Notebooks/Computer_Vision/classification/traffic_signal/model/traffic_vgg16_120.h5')"
      ],
      "metadata": {
        "id": "Ncq1BD7NtC8Q"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Evaluate on test data\")\n",
        "results = model.evaluate(test_generator,batch_size=5)\n",
        "print(\"test loss, test acc:\", results)"
      ],
      "metadata": {
        "id": "leDde9wf4ely",
        "outputId": "7620c5b8-dd05-439d-d875-a1aa44b1e351",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluate on test data\n",
            "7/7 [==============================] - 11s 2s/step - loss: 0.3168 - accuracy: 0.9288\n",
            "test loss, test acc: [0.3168126940727234, 0.9288214445114136]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  "
      ],
      "metadata": {
        "id": "g8H5R2Gb8-T5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}